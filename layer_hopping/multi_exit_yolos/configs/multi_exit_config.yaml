# Multi-Exit YOLOS Fine-tuning Configuration
# For pedestrian detection on MOT17 dataset

# ============================================================================
# Model Configuration
# ============================================================================
model:
  backbone: "hustvl/yolos-small"  # Pretrained YOLOS model
  hidden_dim: 384
  num_classes: 1  # Only person class for MOT17
  num_detection_tokens: 100

  # Exit layer configuration
  exit_layers: [8, 10, 12]  # Multi-exit points

  # Detection head architecture (3-layer MLP)
  head_architecture:
    input_dim: 384
    hidden_dim: 384
    num_layers: 3  # 3-layer MLP with ReLU

# ============================================================================
# Dataset Configuration
# ============================================================================
dataset:
  name: "MOT17"
  root_path: "/ssd_4TB/divake/uncertain_skip/data/MOT17/train"

  # Training sequences
  train_sequences:
    - "MOT17-02-FRCNN"
    - "MOT17-04-FRCNN"
    - "MOT17-05-FRCNN"
    - "MOT17-09-FRCNN"
    - "MOT17-10-FRCNN"

  # Validation sequences
  val_sequences:
    - "MOT17-11-FRCNN"
    - "MOT17-13-FRCNN"

  # Image preprocessing
  image_size: 512  # 512x512 input
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

# ============================================================================
# Loss Configuration (DETR-style)
# ============================================================================
loss:
  # Loss components
  losses: ["labels", "boxes"]  # Classification + bounding box

  # Loss weights (standard DETR weights)
  weight_dict:
    loss_ce: 1.0       # Classification loss (cross-entropy)
    loss_bbox: 5.0     # L1 bounding box loss
    loss_giou: 2.0     # Generalized IoU loss

  # Auxiliary loss weights (same as main loss)
  # Automatically generated for each exit layer
  aux_loss: true

  # Matcher configuration (Hungarian)
  matcher:
    cost_class: 1.0
    cost_bbox: 5.0
    cost_giou: 2.0

  # Empty class weight (no-object)
  eos_coef: 0.1  # Lower weight for "no object" class

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Phase 1: Warm-up (train only new detection heads)
  phase1:
    epochs: 15
    batch_size: 8
    learning_rate: 0.0001  # 1e-4
    weight_decay: 0.0001   # 1e-4
    freeze_backbone: true
    freeze_layer12_head: true  # Keep pretrained Layer 12 head frozen
    description: "Train only Layer 8 and Layer 10 detection heads"

  # Phase 2: Fine-tuning (optional, train everything with small LR)
  phase2:
    enabled: false  # Can enable if Phase 1 results are good
    epochs: 10
    batch_size: 8
    learning_rate: 0.000001  # 1e-6, very small LR for fine-tuning
    weight_decay: 0.0001     # 1e-4
    freeze_backbone: false
    freeze_layer12_head: false
    description: "Fine-tune entire model with small learning rate"

  # Optimizer
  optimizer: "AdamW"
  lr_scheduler: "StepLR"
  lr_drop: 10  # Drop LR every 10 epochs
  lr_drop_rate: 0.1

  # Gradient clipping
  clip_max_norm: 0.1

  # Mixed precision training
  use_amp: true  # Faster training with FP16

  # Checkpointing
  save_every: 5  # Save checkpoint every 5 epochs
  save_best: true  # Save best model based on validation mAP

# ============================================================================
# Evaluation Configuration
# ============================================================================
evaluation:
  # Evaluation frequency
  eval_every_n_epochs: 1  # Evaluate every epoch

  # Metrics to compute
  metrics:
    - "mAP"         # Mean Average Precision
    - "AP50"        # AP at IoU=0.50
    - "AP75"        # AP at IoU=0.75
    - "precision"   # Precision
    - "recall"      # Recall
    - "f1_score"    # F1 score

  # Per-exit evaluation
  evaluate_all_exits: true

  # Confidence threshold for predictions
  conf_threshold: 0.5

  # IoU threshold for matching
  iou_threshold: 0.5

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  # Logging frequency
  log_every_n_steps: 50  # Log every 50 batches

  # Metrics to log
  log_metrics:
    - "loss_total"
    - "loss_ce"
    - "loss_bbox"
    - "loss_giou"
    - "loss_ce_0"   # Layer 8 classification loss
    - "loss_bbox_0" # Layer 8 bbox loss
    - "loss_giou_0" # Layer 8 GIoU loss
    - "loss_ce_1"   # Layer 10 classification loss
    - "loss_bbox_1" # Layer 10 bbox loss
    - "loss_giou_1" # Layer 10 GIoU loss
    - "learning_rate"
    - "grad_norm"

  # Validation metrics to log
  val_metrics:
    - "mAP_layer_8"
    - "mAP_layer_10"
    - "mAP_layer_12"
    - "AP50_layer_8"
    - "AP50_layer_10"
    - "AP50_layer_12"
    - "precision_layer_8"
    - "precision_layer_10"
    - "precision_layer_12"
    - "recall_layer_8"
    - "recall_layer_10"
    - "recall_layer_12"

  # Output directory
  output_dir: "results/multi_exit_training"

  # TensorBoard logging
  use_tensorboard: true
  tensorboard_dir: "results/multi_exit_training/tensorboard"

  # Save detailed logs
  save_train_log: true
  save_val_log: true
  save_loss_curves: true
  save_metric_plots: true

# ============================================================================
# Hardware Configuration
# ============================================================================
hardware:
  device: "cuda"  # Use available GPU (set with CUDA_VISIBLE_DEVICES)
  num_workers: 4     # DataLoader workers
  pin_memory: true   # Faster GPU transfer

  # Distributed training (optional)
  distributed: false
  world_size: 1
  rank: 0

# ============================================================================
# Reproducibility
# ============================================================================
seed: 42
deterministic: true

# ============================================================================
# Experiment Tracking
# ============================================================================
experiment:
  name: "multi_exit_yolos_mot17"
  description: "Fine-tuning YOLOS with exits at layers 8, 10, 12 for pedestrian detection on MOT17"
  tags: ["multi-exit", "yolos", "mot17", "pedestrian-detection", "fine-tuning"]
  notes: |
    Training strategy:
    - Phase 1: Train only new detection heads (layers 8, 10) with frozen backbone
    - Keep pretrained Layer 12 head frozen initially
    - Use DETR-style auxiliary losses
    - Standard loss weights: ce=1, bbox=5, giou=2
