# Extended Phase 1 Training Configuration
# Longer training with frozen backbone and per-epoch evaluation

# ============================================================================
# Model Configuration
# ============================================================================
model:
  backbone: "hustvl/yolos-small"
  hidden_dim: 384
  num_classes: 1
  num_detection_tokens: 100
  exit_layers: [8, 10, 12]

  head_architecture:
    input_dim: 384
    hidden_dim: 384
    num_layers: 3

# ============================================================================
# Dataset Configuration
# ============================================================================
dataset:
  name: "MOT17"
  root_path: "/ssd_4TB/divake/uncertain_skip/data/MOT17/train"

  train_sequences:
    - "MOT17-02-FRCNN"
    - "MOT17-04-FRCNN"
    - "MOT17-05-FRCNN"
    - "MOT17-09-FRCNN"
    - "MOT17-10-FRCNN"

  val_sequences:
    - "MOT17-11-FRCNN"
    - "MOT17-13-FRCNN"

  image_size: 512
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

# ============================================================================
# Loss Configuration
# ============================================================================
loss:
  losses: ["labels", "boxes"]

  weight_dict:
    loss_ce: 1.0
    loss_bbox: 5.0
    loss_giou: 2.0

  # Exit-specific loss weights
  exit_weights:
    layer_8: 0.3
    layer_10: 0.3
    layer_12: 0.4

  aux_loss: true

  matcher:
    cost_class: 1.0
    cost_bbox: 5.0
    cost_giou: 2.0

  eos_coef: 0.1

# ============================================================================
# Extended Phase 1 Training Configuration
# ============================================================================
training:
  epochs: 50  # Extended from 15 to 50
  batch_size: 8

  # Use official YOLOS learning rate for detection heads
  learning_rate: 1.0e-4  # Standard for detection heads
  weight_decay: 0.0001

  # Freeze settings (Phase 1 approach)
  freeze:
    backbone: true            # KEEP FROZEN
    layer_12_bbox: true       # KEEP FROZEN (pretrained)

  # Optimizer
  optimizer: "AdamW"
  lr_scheduler: "StepLR"
  lr_drop: 30  # Drop LR at epoch 30
  lr_drop_rate: 0.1

  # Gradient clipping
  clip_max_norm: 0.1

  # Disable mixed precision to avoid numerical issues
  use_amp: false  # FP32 training for stability

  # Checkpointing
  save_every: 10
  save_best: true

  # Start from Phase 1 best checkpoint
  pretrained_checkpoint: "results/multi_exit_training/20251020_165209/best_model.pt"

# ============================================================================
# Evaluation Configuration
# ============================================================================
evaluation:
  # Evaluate every epoch
  eval_during_training: true
  eval_every_n_epochs: 1

  metrics:
    - "mAP"
    - "AP50"
    - "AP75"
    - "precision"
    - "recall"
    - "f1_score"

  evaluate_all_exits: true
  conf_threshold: 0.1
  iou_threshold: 0.5

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  log_every_n_steps: 50

  log_metrics:
    - "loss_total"
    - "loss_ce"
    - "loss_bbox"
    - "loss_giou"
    - "loss_ce_0"
    - "loss_bbox_0"
    - "loss_giou_0"
    - "loss_ce_1"
    - "loss_bbox_1"
    - "loss_giou_1"
    - "learning_rate"
    - "grad_norm"

  val_metrics:
    - "mAP_layer_8"
    - "mAP_layer_10"
    - "mAP_layer_12"
    - "AP50_layer_8"
    - "AP50_layer_10"
    - "AP50_layer_12"
    - "f1_score_layer_8"
    - "f1_score_layer_10"
    - "f1_score_layer_12"

  output_dir: "results/extended_phase1_training"
  save_train_log: true
  save_val_log: true
  save_loss_curves: true
  save_metric_plots: true

# ============================================================================
# Hardware Configuration
# ============================================================================
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  distributed: false

# ============================================================================
# Reproducibility
# ============================================================================
seed: 42
deterministic: true

# ============================================================================
# Experiment Info
# ============================================================================
experiment:
  name: "extended_phase1_multi_exit_yolos"
  description: "Extended Phase 1: Train detection heads for 50 epochs with frozen backbone and per-epoch evaluation"
  tags: ["extended-phase1", "frozen-backbone", "long-training", "fp32"]
  notes: |
    Extended Phase 1 Training Strategy:
    - Load best checkpoint from initial Phase 1 (epoch 6)
    - Train for 50 epochs total (extended from 15)
    - Keep backbone frozen (avoid NaN issues)
    - Use FP32 training (no mixed precision for stability)
    - Learning rate: 1e-4 (official YOLOS value for detection heads)
    - LR drop at epoch 30 (reduce to 1e-5)
    - Evaluate all exits every epoch
    - Save only when mAP improves

    Expected improvements over initial Phase 1:
    - Layer 8: 1.34% → 5-10% mAP (more epochs for convergence)
    - Layer 10: 4.24% → 10-15% mAP
    - Layer 12: 21.04% → 25-30% mAP

    Rationale:
    - Phase 1 losses still decreasing at epoch 15
    - Longer training allows detection heads to fully optimize
    - Frozen backbone avoids numerical instability
    - FP32 ensures stable gradients throughout training
